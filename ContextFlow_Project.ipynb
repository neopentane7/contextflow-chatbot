{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ContextFlow Hybrid Chatbot Project\n",
    "\n",
    "A sophisticated hybrid chatbot system combining **LSTM** and **Transformer** architectures with advanced inference strategies.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Project Overview](#overview)\n",
    "2. [Architecture](#architecture)\n",
    "3. [Data Preparation](#data)\n",
    "4. [Model Implementation](#model)\n",
    "5. [Training Pipeline](#training)\n",
    "6. [Advanced Inference](#inference)\n",
    "7. [Web Application](#webapp)\n",
    "8. [Deployment](#deployment)\n",
    "9. [Testing & Evaluation](#testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Project Overview <a id=\"overview\"></a>\n",
    "\n",
    "### Features\n",
    "- **Hybrid Model**: Combines LSTM for sequential processing and Transformer for attention mechanisms\n",
    "- **Advanced Inference**: Beam Search, Temperature Sampling, Constrained Decoding\n",
    "- **Context Awareness**: Maintains conversation history and context\n",
    "- **Web Interface**: Flask backend with responsive UI\n",
    "- **Docker Support**: Containerized deployment\n",
    "\n",
    "### Tech Stack\n",
    "- **Deep Learning**: PyTorch\n",
    "- **Backend**: Flask\n",
    "- **Frontend**: HTML/CSS/JavaScript\n",
    "- **Deployment**: Docker, Docker Compose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Architecture <a id=\"architecture\"></a>\n",
    "\n",
    "### Hybrid Model Architecture\n",
    "\n",
    "```\n",
    "Input Text\n",
    "    ↓\n",
    "Tokenization\n",
    "    ↓\n",
    "Embedding Layer (256-dim)\n",
    "    ↓\n",
    "LSTM Layers (2 layers, 512 hidden)\n",
    "    ↓\n",
    "Transformer Layers (4 layers, 8 heads)\n",
    "    ↓\n",
    "Output Layer (vocab_size)\n",
    "    ↓\n",
    "Generated Response\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Preparation <a id=\"data\"></a>\n",
    "\n",
    "### Custom Tokenizer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"Custom tokenizer for chatbot training\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word2idx = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
    "        self.vocab_size = 4\n",
    "    \n",
    "    def fit(self, texts):\n",
    "        \"\"\"Build vocabulary from texts\"\"\"\n",
    "        for text in texts:\n",
    "            tokens = text.lower().split()\n",
    "            for token in tokens:\n",
    "                if token not in self.word2idx:\n",
    "                    self.word2idx[token] = self.vocab_size\n",
    "                    self.idx2word[self.vocab_size] = token\n",
    "                    self.vocab_size += 1\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert text to token IDs\"\"\"\n",
    "        tokens = text.lower().split()\n",
    "        return [self.word2idx.get(token, self.word2idx['<UNK>']) for token in tokens]\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Convert token IDs back to text\"\"\"\n",
    "        tokens = [self.idx2word.get(idx, '<UNK>') for idx in token_ids]\n",
    "        return ' '.join([t for t in tokens if t not in ['<PAD>', '<SOS>', '<EOS>']])\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save tokenizer to file\"\"\"\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        \"\"\"Load tokenizer from file\"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    def sos_id(self):\n",
    "        return self.word2idx['<SOS>']\n",
    "    \n",
    "    def eos_id(self):\n",
    "        return self.word2idx['<EOS>']\n",
    "    \n",
    "    def pad_id(self):\n",
    "        return self.word2idx['<PAD>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "data_path = 'data/merged_training_data.csv'\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Loaded {len(df)} conversation pairs\")\n",
    "    print(\"\\nSample data:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Data statistics\n",
    "    print(\"\\n=== Data Statistics ===\")\n",
    "    print(f\"Total pairs: {len(df)}\")\n",
    "    if 'input' in df.columns and 'response' in df.columns:\n",
    "        print(f\"Avg input length: {df['input'].str.split().str.len().mean():.2f} words\")\n",
    "        print(f\"Avg response length: {df['response'].str.split().str.len().mean():.2f} words\")\n",
    "else:\n",
    "    print(f\"Data file not found at {data_path}\")\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    \n",
    "    # Sample data for demonstration\n",
    "    sample_data = {\n",
    "        'input': [\n",
    "            'hello',\n",
    "            'what is machine learning',\n",
    "            'how are you',\n",
    "            'what is deep learning',\n",
    "            'explain neural networks'\n",
    "        ],\n",
    "        'response': [\n",
    "            'Hello! How can I help you?',\n",
    "            'Machine learning is a field of AI where computers learn from data',\n",
    "            'I am doing well, thanks for asking!',\n",
    "            'Deep learning uses neural networks with multiple layers',\n",
    "            'Neural networks are computing systems inspired by biological brains'\n",
    "        ]\n",
    "    }\n",
    "    df = pd.DataFrame(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "if 'input' in df.columns and 'response' in df.columns:\n",
    "    all_texts = df['input'].tolist() + df['response'].tolist()\n",
    "    tokenizer.fit(all_texts)\n",
    "    print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "    print(f\"Sample tokens: {list(tokenizer.word2idx.keys())[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataset(Dataset):\n",
    "    \"\"\"Custom dataset for chatbot training\"\"\"\n",
    "    \n",
    "    def __init__(self, inputs, responses, tokenizer, max_length=50):\n",
    "        self.inputs = inputs\n",
    "        self.responses = responses\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.inputs[idx]\n",
    "        response_text = self.responses[idx]\n",
    "        \n",
    "        # Encode\n",
    "        input_ids = self.tokenizer.encode(input_text)\n",
    "        response_ids = [self.tokenizer.sos_id()] + self.tokenizer.encode(response_text) + [self.tokenizer.eos_id()]\n",
    "        \n",
    "        # Pad or truncate\n",
    "        input_ids = input_ids[:self.max_length]\n",
    "        response_ids = response_ids[:self.max_length]\n",
    "        \n",
    "        input_ids += [self.tokenizer.pad_id()] * (self.max_length - len(input_ids))\n",
    "        response_ids += [self.tokenizer.pad_id()] * (self.max_length - len(response_ids))\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'response_ids': torch.tensor(response_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create dataset\n",
    "if 'input' in df.columns and 'response' in df.columns:\n",
    "    dataset = ChatbotDataset(\n",
    "        df['input'].tolist(),\n",
    "        df['response'].tolist(),\n",
    "        tokenizer,\n",
    "        max_length=50\n",
    "    )\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    print(f\"Created dataset with {len(dataset)} samples\")\n",
    "    print(f\"Batch size: 32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model Implementation <a id=\"model\"></a>\n",
    "\n",
    "### Hybrid LSTM-Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridChatbotModel(nn.Module):\n",
    "    \"\"\"Hybrid model combining LSTM and Transformer architectures\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=256, hidden_dim=512, \n",
    "                 num_layers=2, num_heads=8, dropout=0.1):\n",
    "        super(HybridChatbotModel, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # LSTM layers for sequential processing\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        \n",
    "        # Transformer encoder layers for attention\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # LSTM processing\n",
    "        if hidden is None:\n",
    "            lstm_out, hidden = self.lstm(embedded)\n",
    "        else:\n",
    "            lstm_out, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # Transformer processing\n",
    "        transformer_out = self.transformer(lstm_out)\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.fc_out(transformer_out)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"Initialize hidden states\"\"\"\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return (h0, c0)\n",
    "\n",
    "# Model configuration\n",
    "MODEL_CONFIG = {\n",
    "    'embedding_dim': 256,\n",
    "    'hidden_dim': 512,\n",
    "    'num_layers': 2,\n",
    "    'num_heads': 8,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "model = HybridChatbotModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    **MODEL_CONFIG\n",
    ").to(device)\n",
    "\n",
    "# Model summary\n",
    "print(\"=== Model Architecture ===\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Training Pipeline <a id=\"training\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "CLIP_GRAD = 1.0\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id())\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Gradient clipping: {CLIP_GRAD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, clip_grad=1.0):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc='Training')\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        response_ids = batch['response_ids'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(response_ids[:, :-1])\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(\n",
    "            output.reshape(-1, output.shape[-1]),\n",
    "            response_ids[:, 1:].reshape(-1)\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            response_ids = batch['response_ids'].to(device)\n",
    "            \n",
    "            output = model(response_ids[:, :-1])\n",
    "            loss = criterion(\n",
    "                output.reshape(-1, output.shape[-1]),\n",
    "                response_ids[:, 1:].reshape(-1)\n",
    "            )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "train_losses = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "print(\"\\n=== Starting Training ===\")\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, dataloader, criterion, optimizer, device, CLIP_GRAD)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(train_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        os.makedirs('models/checkpoints', exist_ok=True)\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': train_loss,\n",
    "        }, f'models/checkpoints/checkpoint_epoch_{epoch + 1}.pt')\n",
    "        print(f\"✅ Saved checkpoint (best loss: {best_loss:.4f})\")\n",
    "\n",
    "print(\"\\n=== Training Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, marker='o', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training Loss Over Epochs', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Advanced Inference <a id=\"inference\"></a>\n",
    "\n",
    "### Inference Strategies\n",
    "\n",
    "1. **Beam Search**: Explores multiple hypotheses in parallel\n",
    "2. **Temperature Sampling**: Generates diverse, creative responses\n",
    "3. **Constrained Decoding**: Avoids forbidden words/phrases\n",
    "4. **Ensemble Method**: Combines multiple strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedInference:\n",
    "    \"\"\"Advanced inference strategies for better response generation\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device='cuda'):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "    \n",
    "    def beam_search(self, input_text, beam_width=5, max_length=50, temperature=0.7):\n",
    "        \"\"\"\n",
    "        Beam Search decoding - explores multiple hypotheses in parallel\n",
    "        \n",
    "        Advantages:\n",
    "        - Better quality responses\n",
    "        - Explores diverse paths\n",
    "        - More natural language\n",
    "        \"\"\"\n",
    "        input_ids = torch.tensor([self.tokenizer.encode(input_text)], dtype=torch.long).to(self.device)\n",
    "        \n",
    "        # Initialize beams\n",
    "        initial_tokens = input_ids[0].tolist()\n",
    "        beams = [{'tokens': initial_tokens, 'score': 0.0}]\n",
    "        completed_beams = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for step in range(max_length):\n",
    "                candidates = []\n",
    "                \n",
    "                for beam in beams:\n",
    "                    beam_tokens = torch.tensor([beam['tokens']], dtype=torch.long).to(self.device)\n",
    "                    \n",
    "                    # Get model output\n",
    "                    output = self.model(beam_tokens)\n",
    "                    logits = output[0, -1, :] / temperature\n",
    "                    \n",
    "                    # Get top-k probabilities\n",
    "                    probs = torch.softmax(logits, dim=-1)\n",
    "                    top_probs, top_indices = torch.topk(probs, beam_width)\n",
    "                    \n",
    "                    for prob, token_id in zip(top_probs, top_indices):\n",
    "                        new_beam = {\n",
    "                            'tokens': beam['tokens'] + [token_id.item()],\n",
    "                            'score': beam['score'] + torch.log(prob).item()\n",
    "                        }\n",
    "                        candidates.append(new_beam)\n",
    "                \n",
    "                # Keep top beam_width candidates\n",
    "                candidates.sort(key=lambda x: x['score'], reverse=True)\n",
    "                beams = candidates[:beam_width]\n",
    "                \n",
    "                # Check if we should stop\n",
    "                if all(beam['tokens'][-1] == self.tokenizer.eos_id() for beam in beams):\n",
    "                    completed_beams = beams\n",
    "                    break\n",
    "            \n",
    "            if not completed_beams:\n",
    "                completed_beams = beams\n",
    "        \n",
    "        # Decode best beam\n",
    "        best_tokens = completed_beams[0]['tokens'][len(initial_tokens):]\n",
    "        response = self.tokenizer.decode(best_tokens)\n",
    "        return response\n",
    "    \n",
    "    def temperature_sampling(self, input_text, temperature=0.8, top_k=50, \n",
    "                           top_p=0.9, max_length=50):\n",
    "        \"\"\"\n",
    "        Temperature Sampling - generates diverse, creative responses\n",
    "        \n",
    "        Parameters:\n",
    "        - temperature: 0.0 = deterministic, 1.0 = normal, >1.0 = creative\n",
    "        - top_k: Keep only top K tokens\n",
    "        - top_p: Keep tokens with cumulative prob <= p (nucleus sampling)\n",
    "        \"\"\"\n",
    "        input_ids = torch.tensor([self.tokenizer.encode(input_text)], dtype=torch.long).to(self.device)\n",
    "        \n",
    "        generated_tokens = []\n",
    "        current_input = input_ids.clone()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                output = self.model(current_input)\n",
    "                logits = output[0, -1, :] / temperature\n",
    "                \n",
    "                # Top-K filtering\n",
    "                if top_k > 0:\n",
    "                    indices_to_remove = torch.topk(logits, top_k, largest=False).indices\n",
    "                    logits[indices_to_remove] = float('-inf')\n",
    "                \n",
    "                # Top-P (Nucleus) filtering\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumsum_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=0)\n",
    "                sorted_indices_to_remove = cumsum_probs > top_p\n",
    "                sorted_indices_to_remove[0] = False\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[indices_to_remove] = float('-inf')\n",
    "                \n",
    "                # Sample\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1).item()\n",
    "                \n",
    "                generated_tokens.append(next_token)\n",
    "                \n",
    "                if next_token == self.tokenizer.eos_id():\n",
    "                    break\n",
    "                \n",
    "                current_input = torch.cat([\n",
    "                    current_input,\n",
    "                    torch.tensor([[next_token]], dtype=torch.long).to(self.device)\n",
    "                ], dim=1)\n",
    "        \n",
    "        response = self.tokenizer.decode(generated_tokens)\n",
    "        return response\n",
    "\n",
    "# Initialize inference engine\n",
    "inference_engine = AdvancedInference(model, tokenizer, device)\n",
    "print(\"✅ Inference engine initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Inference Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different inference methods\n",
    "test_input = \"what is machine learning\"\n",
    "\n",
    "print(f\"Input: {test_input}\\n\")\n",
    "\n",
    "# Beam Search\n",
    "print(\"=== Beam Search ===\")\n",
    "response_beam = inference_engine.beam_search(test_input, beam_width=5)\n",
    "print(f\"Response: {response_beam}\\n\")\n",
    "\n",
    "# Temperature Sampling\n",
    "print(\"=== Temperature Sampling ===\")\n",
    "response_temp = inference_engine.temperature_sampling(test_input, temperature=0.8)\n",
    "print(f\"Response: {response_temp}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Web Application <a id=\"webapp\"></a>\n",
    "\n",
    "### Flask Backend Structure\n",
    "\n",
    "```python\n",
    "# backend/app_advanced.py\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "from backend.inference import AdvancedInference\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Initialize model and inference engine\n",
    "# ...\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/api/chat', methods=['POST'])\n",
    "def chat():\n",
    "    data = request.json\n",
    "    user_message = data.get('message', '')\n",
    "    method = data.get('method', 'beam_search')\n",
    "    \n",
    "    # Generate response\n",
    "    if method == 'beam_search':\n",
    "        response = inference_engine.beam_search(user_message)\n",
    "    else:\n",
    "        response = inference_engine.temperature_sampling(user_message)\n",
    "    \n",
    "    return jsonify({'bot_response': response})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Endpoints\n",
    "\n",
    "#### POST /api/chat\n",
    "\n",
    "**Request:**\n",
    "```json\n",
    "{\n",
    "    \"message\": \"Hello\",\n",
    "    \"session_id\": \"user_123\",\n",
    "    \"method\": \"beam_search\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Response:**\n",
    "```json\n",
    "{\n",
    "    \"bot_response\": \"Hello! How can I help you?\",\n",
    "    \"session_id\": \"user_123\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Deployment <a id=\"deployment\"></a>\n",
    "\n",
    "### Docker Configuration\n",
    "\n",
    "#### Dockerfile\n",
    "```dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY backend/requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY . .\n",
    "\n",
    "EXPOSE 5000\n",
    "\n",
    "CMD [\"python\", \"backend/app_advanced.py\"]\n",
    "```\n",
    "\n",
    "#### docker-compose.yml\n",
    "```yaml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  chatbot:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"5000:5000\"\n",
    "    volumes:\n",
    "      - ./models:/app/models\n",
    "    environment:\n",
    "      - FLASK_ENV=production\n",
    "```\n",
    "\n",
    "### Deployment Commands\n",
    "\n",
    "```bash\n",
    "# Build and run with Docker Compose\n",
    "docker-compose up --build\n",
    "\n",
    "# Access at http://localhost:5000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Testing & Evaluation <a id=\"testing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive testing\n",
    "def interactive_chat():\n",
    "    \"\"\"Interactive chat session\"\"\"\n",
    "    print(\"\\n=== ContextFlow Chatbot ===\")\n",
    "    print(\"Type 'quit' to exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        # Generate response\n",
    "        response = inference_engine.beam_search(user_input)\n",
    "        print(f\"Bot: {response}\\n\")\n",
    "\n",
    "# Uncomment to run interactive chat\n",
    "# interactive_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch testing\n",
    "test_cases = [\n",
    "    \"hello\",\n",
    "    \"what is machine learning\",\n",
    "    \"how are you\",\n",
    "    \"explain deep learning\",\n",
    "    \"what is a neural network\"\n",
    "]\n",
    "\n",
    "print(\"=== Batch Testing ===\")\n",
    "for i, test_input in enumerate(test_cases, 1):\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(f\"Input: {test_input}\")\n",
    "    \n",
    "    response = inference_engine.beam_search(test_input)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Measure inference time\n",
    "def measure_inference_time(inference_engine, test_input, method='beam_search', num_runs=10):\n",
    "    \"\"\"Measure average inference time\"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if method == 'beam_search':\n",
    "            _ = inference_engine.beam_search(test_input)\n",
    "        else:\n",
    "            _ = inference_engine.temperature_sampling(test_input)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    \n",
    "    print(f\"\\n=== Inference Time ({method}) ===\")\n",
    "    print(f\"Average: {avg_time:.4f}s\")\n",
    "    print(f\"Std Dev: {std_time:.4f}s\")\n",
    "    print(f\"Min: {min(times):.4f}s\")\n",
    "    print(f\"Max: {max(times):.4f}s\")\n",
    "    \n",
    "    return times\n",
    "\n",
    "# Measure performance\n",
    "test_input = \"what is machine learning\"\n",
    "beam_times = measure_inference_time(inference_engine, test_input, 'beam_search', num_runs=5)\n",
    "temp_times = measure_inference_time(inference_engine, test_input, 'temperature_sampling', num_runs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize inference times\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Beam Search times\n",
    "ax[0].bar(range(1, len(beam_times) + 1), beam_times, color='steelblue', alpha=0.7)\n",
    "ax[0].axhline(np.mean(beam_times), color='red', linestyle='--', label=f'Mean: {np.mean(beam_times):.4f}s')\n",
    "ax[0].set_xlabel('Run', fontsize=11)\n",
    "ax[0].set_ylabel('Time (seconds)', fontsize=11)\n",
    "ax[0].set_title('Beam Search Inference Time', fontsize=13, fontweight='bold')\n",
    "ax[0].legend()\n",
    "ax[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Temperature Sampling times\n",
    "ax[1].bar(range(1, len(temp_times) + 1), temp_times, color='coral', alpha=0.7)\n",
    "ax[1].axhline(np.mean(temp_times), color='red', linestyle='--', label=f'Mean: {np.mean(temp_times):.4f}s')\n",
    "ax[1].set_xlabel('Run', fontsize=11)\n",
    "ax[1].set_ylabel('Time (seconds)', fontsize=11)\n",
    "ax[1].set_title('Temperature Sampling Inference Time', fontsize=13, fontweight='bold')\n",
    "ax[1].legend()\n",
    "ax[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrates the complete ContextFlow Hybrid Chatbot project:\n",
    "\n",
    "1. ✅ **Custom Tokenizer** - Built vocabulary from training data\n",
    "2. ✅ **Hybrid Model** - Combined LSTM and Transformer architectures\n",
    "3. ✅ **Training Pipeline** - Trained model with gradient clipping and learning rate scheduling\n",
    "4. ✅ **Advanced Inference** - Implemented Beam Search and Temperature Sampling\n",
    "5. ✅ **Web Application** - Flask backend with REST API\n",
    "6. ✅ **Docker Deployment** - Containerized application\n",
    "7. ✅ **Testing & Evaluation** - Performance metrics and benchmarking\n",
    "\n",
    "### Key Features\n",
    "- **Hybrid Architecture**: Combines sequential (LSTM) and attention (Transformer) mechanisms\n",
    "- **Multiple Inference Strategies**: Beam Search, Temperature Sampling, Constrained Decoding\n",
    "- **Context Awareness**: Maintains conversation history\n",
    "- **Production Ready**: Docker deployment with Flask backend\n",
    "\n",
    "### Next Steps\n",
    "1. Fine-tune hyperparameters\n",
    "2. Expand training dataset\n",
    "3. Implement more advanced context management\n",
    "4. Add user authentication\n",
    "5. Deploy to cloud platform (AWS, GCP, Azure)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
